\section{Research Questions and Operationalization}\label{ch:operationalization}

In the following, we introduce our research questions and formalize how we evaluate them. Afterward,
we explain how we collect our data we use for the evaluation and introduce the experiment we conduct. 
The goal of this work is to work out the differences between white-box and black-box models when analyzing the feature performance of configurable
software systems. 
To quantify these differences, we want to answer the following research question:\\

\noindent \textbf{RQ1}: How accurately do white-box and black-box models detect features and feature interactions? \\

%Motivation
Due to the number of features, it is hard to know the influence of each feature or feature interaction on a configurable system. 
Therefore, when we use white-box or black-box analysis to quantify the influence of these features and feature interaction, 
we are interested in the accuracy of both analyses. 
Another point of interest is if we can identify a group of features for which either analysis performs worse than its counterpart.
To answer this, we research how accurately they can identify the influence of features and feature interactions.

%Reason Mape
Thus, we quantify the difference between the predicted and true influence of each feature or feature interaction $f$ 
in the set of features and feature interactions $F$ by calculating $error_f$ and afterwards its mean $\overline{error}$, 
following the approach outlined in~\cite{mape}. %Maybe reference


\begin{align}
    error_f &= \frac{\lvert actual\_true_f - predicted_f \rvert}{actual\_true_f} \label{equ:APE_RQ1} \\ \nonumber \\
    \overline{error}  &= \frac{\sum_{f \in F} error_f}{\lvert F \rvert} \label{equ:MAPE_RQ1}
\end{align}

Note that $actual\_true_f$ is the true influence of the feature or feature interaction $f$ that we obtain from the baseline of our ground truth.
Whereas $predicted_f$ is the influence predicted by the {\perfInfluenceModel}s we build.
The closer $\overline{error}$ is to $0$, the better the prediction, with an $\overline{error}$ of 0 indicating a perfectly
accurate \perfInfluenceModel. 

%How we reasearch this
To investigate this, we use a qualitative measurement using the ground truth systems we design. 
We build a {\perfInfluenceModel} with each analysis method, and use \autoref{equ:MAPE_RQ1},
we know the actual values for each feature and feature interaction.

Another, point of interest is how similar the {\perfInfluenceModel}s are. We answer this in the following research question.\\

\noindent \textbf{RQ2}: Do performance models created by our white-box and black-box attribute the same influence to each feature?\\

%Motivation
This is of interest because, because if both analyses produce similar results we can use them both interchangeable with each other and 
therefore choose the analysis we prefer without compromising its accuracy.

%How we anwser
To answer this question, we investigate whether the {\perfInfluenceModel}s build-out of the white-box and black-box analyses data agree, i.e.
they predict the same influence for each feature. 
Compared to RQ1, we do not measure if the predictions are accurate regarding the true influence but if both analyses produce similar results. 
To quantify the difference, we use the following formulas:

\begin{align}
    similarity_f &= \frac{\lvert predicted^{WB}_{f} - predicted^{BB}_{f} \rvert}{\Pi_{BB}} \label{equ:APE_RQ1} \\ \nonumber \\
    \overline{similarity_f}  &=  \frac{\sum_{f \in F} similarity_f}{\lvert F \rvert} \label{equ:MAPE_RQ1}
\end{align}

Here $WB$ stands for white box and $BB$ for black box. How similar two features are is calculated by $similarity_f$, 
where we normalize the absolute difference of the time prediction for each feature by the overall time for the black-box {\perfInfluenceModel}s. 
Ideally, the overall time for both {\perfInfluenceModel}s should be the same, with only the time distribution between features being different. 
However, the measurement code creates an overhead, which is minimal for the black-box analysis since we only measure when the system starts and when it ends.
Afterward, we calculate the mean similarity $\overline{similarity_f}$ by dividing through the number of features and feature interactions $F$. 
A $\overline{similarity_f}$ of 0 indicates that both {\perfInfluenceModel}s are perfectly similar.
%Maybe aussage von mean sim begr√ºnden also was die werte aussagen 0=perfekt, 1 = komplett unterschiedlich

To evaluate this question, we use both qualitative and quantitative measurements. 
For the qualitative measurements, we use the ground truth systems we design to identify why the {\perfInfluenceModel}s differ. 
For the quantitative measurements, we use experimental results to investigate the reasons for similarities and differences.

\section{Collecting Data}\label{ch:collect-data}
%Intro
Now that we have defined how we answer the research questions, we explain how we collect the data using both analyses.

%NFP we measure and why
The non-functional property that we decide to analyze is runtime, due to it being a quantifiable metric
that reflects the changes to the system if a feature with a significant influence is selected.

%Repetition and why we need it
When measuring with either white-box or black-box analysis, 
each measurement is subject to noise, which influences the performance during the execution of the system. 
To reduce this noise, we follow the suggestions made by Arcuri et.al~\cite{SampleSize}, 
measure each configuration 30 times, and take the mean of the time spent as values to build the {\perfInfluenceModel}.

%How we collect measurements using VaraTS
We collect the data for each analysis using the VaRA Tool Suite, in which we define an experiment for each project. 
For our white-box analysis, we write an experiment that specifies that we want to use VaRA to instrument our code so that during execution, 
all trace events can be tracked and written into the TEF report file. For our black-box analysis, 
we wrap the command we want to measure using the Linux \texttt{time} command and write the time spent into a different report. 

%Server spezifikation
All measurements are performed on the same server node. To minimize background noise, we ensured that no other job is executed during the measurement. 
The server node contains an AMD EPYC $72$F$3$  $8$-Core Processor with $16$ threads and $256$ GB RAM.

%Libarys Mention
During evaluation, we use different libraries, we will mention the noteworthy ones.
To build the {\perfInfluenceModel} we the implementation of multiple linear regression by \textsc{scikit-learn}\footnote{Visited at 03.04.2023\\ \url{https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html}}.
In addition to that we use the implementation of the variance inflation factor implemented in the \textsc{statsmodels} library\footnote{Visited at 03.04.2023\\ \url{https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html}}.
%%%%%%%%%%%%%%%%

\input{Chapters/Thesis/Experiments.tex}