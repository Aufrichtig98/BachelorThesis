\section{Research Questions and Operationalization}\label{ch:operationalization}

In the following, we introduce our \hyperref[researchQuestions]{research questions} and formalize how we evaluate them. 
The goal of this work is to work out the differences between white-box and black-box models when analyzing the feature performance of configurable
software systems. 
To quantify these differences, ask:\\

\noindent \textbf{RQ1}: How accurately do white-box and black-box models detect features and feature interactions? \\

%Motivation
Due to the sheer number of features, it is hard to know the influence each feature or feature interaction has on a configurable system. 
Therefore, when we use white-box or black-box analysis to quantify the influence of these features and feature interaction, 
we are interested in the accuracy of both analyses. 
Another point of interest is if we can identify a group of features for which either analysis performs worse than its counterpart.
To answer this, we research how accurately they can identify the influence of features and feature interactions.

%How we reasearch this
To investigate this we design multiple small configurable systems in the ground truth section. 
We use the ground truth systems for this because, in these systems, 
we are fully aware of how much time we should spend in each feature or feature interaction. 
%Reason Mape
Thus, we quantify the difference between the predicted and true influence of each feature $f$ in the set of features and feature interactions $F$
by calculating the \emph{error rate} and afterwards its mean, following the approach outlined in~\cite{mape}. %Maybe reference


\begin{align}
    error_f &= \frac{\lvert true_f - predicted_f \rvert}{true_f} \label{equ:APE_RQ1} \\ \nonumber \\
    \overline{error}  &= \frac{\sum_{f \in F} error_f}{\lvert F \rvert} \label{equ:MAPE_RQ1}
\end{align}

Note that $true_f$ is the true influence of the feature or feature interaction $f$ that we obtain from the baseline of our ground truth 
and $predicted_f$ is the influence predicted by the {\perfInfluenceModel}s we build.
The closer $\overline{error}$ is to $0$, the better the prediction, with an $\overline{error}$ of 0 indicating a perfectly
accurate \perfInfluenceModel. 

Another, point of interest is how similar the {\perfInfluenceModel}s are, we anwser this in the following research question.\\

\noindent \textbf{RQ2}: Do performance models created by our white-box and black-box attribute the same influence to each feature?\\

%Motivation
Another point we are interested in is to know if the {\perfInfluenceModel}s of both analyses are similar.
This is of interest because, similar analyses that we can use both analyses interchangeably for one another.

%How we anwser
To answer this question, we investigate whether the {\perfInfluenceModel}s build-out of the white-box and black-box analyses' data agree, i.e.
they predict the same influence for each feature. 
Compared to RQ1, we do not measure if the predictions are accurate regarding the true influence but if both analyses produce similar results. 
To quantify the difference, we again use the mean error rate. However, we adjust the formulas as follows:

\begin{align}
    similarity_f &= \frac{\lvert predicted_{f, Model_1} - predicted_{f, Model_2} \rvert}{predicted_{f, Model_1}} \label{equ:APE_RQ1} \\ \nonumber \\
    \overline{similarity_f}  &= \frac{\sum_{f \in F} error_f}{\lvert F \rvert} \label{equ:MAPE_RQ1}
\end{align}

Here we quantify the similarity between the predictions of models for each feature \emph{f}. Since the similarity between
white-box and black-box model is not the same as the similarity between black-box and white-box model, we have to calculate it for both models.

\mycomment{
\noindent \textbf{RQ3}: Given the same configuration do performance models created by our white-box and black-box predict the same value? \\

At last, we are interested if the prediction for all configurations we measured are similar. Which helps us to identify if the 

To answer this research question, we investigate whether the {\perfInfluenceModel}s build-out of the white-box and black-box analyses' predict
the same values for the same configurations. In comparison to RQ2 we compare how similar the predictions for each configuration are instead
of the prediction for each feature. Therefore, we calculate the \emph{error rate} for each configuration $c \in C$ and afterward the 
mean \emph{error rate} for $C$, by using the following formulas:

\begin{align}
    \text{APE}_{RQ3}(c) &= \frac{\lvert \Pi_{WB}(c) - \widehat{\Pi}_{BB}(c)\rvert}{\Pi_{WB}(c)} \label{equ:APE_RQ3} \\ \nonumber \\
    \text{MAPE}_{RQ3}(C) &= \frac{\sum_{c \in C} \text{APE}(c)}{\lvert C \rvert} \label{equ:MAPE_RQ3}
\end{align}

For this research question the $\overline{error}$ quantifys how similar the predictions of the {\perfInfluenceModel}s are for the same configurations.
}

\section{Collecting Data}\label{ch:collect-data}
%Intro
Now that we have defined how we answer the research questions, we explain on how we collect the data using both analyses.

%NFP we measure and why
The non-functional property that we decide to analyze is runtime, due to it being a quantifiable metric
that reflects the changes to the system if a feature with a significant influence is selected.

%Repetition and why we need it
When measuring with either white-box or black-box analysis, 
each measurement is subject to noise, which influences the performance during the execution of the system. 
To reduce this noise, we follow the suggestions made by Arcuri et.al~\cite{SampleSize}, 
measure each configuration 30 times, and take the mean of the time spent as values to build the {\perfInfluenceModel}.

%How we collect measurements using VaraTS
We collect the data for each analysis using the VaRA Tool Suite, in which we define an experiment for each project. 
For our white-box analysis, we write an experiment that specifies that we want to use VaRA to instrument our code so that during execution, 
all trace events can be tracked and written into the TEF report file. For our black-box analysis, 
we wrap the command we want to measure using the Linux \texttt{time} command and write the time spent into a different report. 

%Server spezifikation
All the measurements are performed on the same server node. To minimize background noise, we ensured that no other job is executed during the measurement. 
The server node contains an AMD EPYC 72F3 8-Core Processor with 16 threads and 256 GB RAM.

%Libarys Mention
During evaluation, we use different libraries, we will mention the noteworthy ones.
To build the {\perfInfluenceModel} we the implementation of multiple linear regression by \textsc{scikit-learn}\footnote{Visited at 03.04.2023 \url{https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html}}.
In addition to that we use the implementation of the variance inflation factor implemented in the \textsc{statsmodels} library\footnote{Visited at 03.04.2023 \url{https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html}}.
%%%%%%%%%%%%%%%%