\section{Research Questions and Operationalization}\label{ch:operationalization}

In the following, we introduce our \hyperref[researchQuestions]{research questions} and formalize how we evaluate them. Afterward,
we explain how we collect our data we use for the evaluation and introduce the experiment we conduct. 
The goal of this work is to work out the differences between white-box and black-box models when analyzing the feature performance of configurable
software systems. 
To quantify these differences, we ask:\\

\noindent \textbf{RQ1}: How accurately do white-box and black-box models detect features and feature interactions? \\

%Motivation
Due to the sheer number of features, it is hard to know the influence each feature or feature interaction has on a configurable system. 
Therefore, when we use white-box or black-box analysis to quantify the influence of these features and feature interaction, 
we are interested in the accuracy of both analyses. 
Another point of interest is if we can identify a group of features for which either analysis performs worse than its counterpart.
To answer this, we research how accurately they can identify the influence of features and feature interactions.

%Reason Mape
Thus, we quantify the difference between the predicted and true influence of each feature $f$ in the set of features and feature interactions $F$
by calculating the \emph{error rate} and afterwards its mean, following the approach outlined in~\cite{mape}. %Maybe reference


\begin{align}
    error_f &= \frac{\lvert true_f - predicted_f \rvert}{true_f} \label{equ:APE_RQ1} \\ \nonumber \\
    \overline{error}  &= \frac{\sum_{f \in F} error_f}{\lvert F \rvert} \label{equ:MAPE_RQ1}
\end{align}

Note that $true_f$ is the true influence of the feature or feature interaction $f$ that we obtain from the baseline of our ground truth 
and $predicted_f$ is the influence predicted by the {\perfInfluenceModel}s we build.
The closer $\overline{error}$ is to $0$, the better the prediction, with an $\overline{error}$ of 0 indicating a perfectly
accurate \perfInfluenceModel. 

%How we reasearch this
To investigate this, we use a qualitative measurement by using the ground truth systems we design. 
We build a {\perfInfluenceModel} with each analysis method, and afterwards use the formula of \autoref{equ:MAPE_RQ1}. 
Since for these systems, we know the actual values for each feature and feature interaction.

Another, point of interest is how similar the {\perfInfluenceModel}s are, we answer this in the following research question.\\

\noindent \textbf{RQ2}: Do performance models created by our white-box and black-box attribute the same influence to each feature?\\

%Motivation
Another point we are interested in is to know if the {\perfInfluenceModel}s of both analyses are similar.
This is of interest because, similar analyses that we can use both analyses interchangeably for one another.

%How we anwser
To answer this question, we investigate whether the {\perfInfluenceModel}s build-out of the white-box and black-box analyses data agree, i.e.
they predict the same influence for each feature. 
Compared to RQ1, we do not measure if the predictions are accurate regarding the true influence but if both analyses produce similar results. 
To quantify the difference, we use the following formulas as follows:

\begin{align}
    similarity_f &= \frac{\lvert predicted^{WB}_{f} - predicted^{BB}_{f} \rvert}{\Pi_{BB}} \label{equ:APE_RQ1} \\ \nonumber \\
    \overline{similarity_f}  &=  \frac{\sum_{f \in F} similarity_f}{\lvert F \rvert} \label{equ:MAPE_RQ1}
\end{align}

Here WB stands for white-box and BB for black-box. How two features are is calculated by $similarity_f$, 
where we normalize the absolute difference of the time prediction for each feature by the overall time for the black-box {\perfInfluenceModel}s. 
Ideally, the overall time for both {\perfInfluenceModel}s should be the same, with only the time distribution between features being different. 
However, the measurement code we use creates an overhead, which is minimal for the black-box analysis since we only measure when we start the system and when it ends.
Afterward, we calculate the mean similarity $overline{similarity_f}$ by dividing through the number of features and feature interactions $F$. 
A $overline{similarity_f}$ of 0 indicates that both {\perfInfluenceModel}s are perfectly similar.
%Maybe aussage von mean sim begr√ºnden also was die werte aussagen 0=perfekt, 1 = komplett unterschiedlich

To evaluate this question, we use both qualitative and quantitative measurements. 
For the qualitative measurements, we use the ground truth systems we design to identify why the {\perfInfluenceModel}s differ. 
For the quantitative measurements, we use experimental results to investigate the reasons for similarities and differences.

\section{Collecting Data}\label{ch:collect-data}
%Intro
Now that we have defined how we answer the research questions, we explain on how we collect the data using both analyses.

%NFP we measure and why
The non-functional property that we decide to analyze is runtime, due to it being a quantifiable metric
that reflects the changes to the system if a feature with a significant influence is selected.

%Repetition and why we need it
When measuring with either white-box or black-box analysis, 
each measurement is subject to noise, which influences the performance during the execution of the system. 
To reduce this noise, we follow the suggestions made by Arcuri et.al~\cite{SampleSize}, 
measure each configuration 30 times, and take the mean of the time spent as values to build the {\perfInfluenceModel}.

%How we collect measurements using VaraTS
We collect the data for each analysis using the VaRA Tool Suite, in which we define an experiment for each project. 
For our white-box analysis, we write an experiment that specifies that we want to use VaRA to instrument our code so that during execution, 
all trace events can be tracked and written into the TEF report file. For our black-box analysis, 
we wrap the command we want to measure using the Linux \texttt{time} command and write the time spent into a different report. 

%Server spezifikation
All the measurements are performed on the same server node. To minimize background noise, we ensured that no other job is executed during the measurement. 
The server node contains an AMD EPYC 72F3 8-Core Processor with 16 threads and 256 GB RAM.

%Libarys Mention
During evaluation, we use different libraries, we will mention the noteworthy ones.
To build the {\perfInfluenceModel} we the implementation of multiple linear regression by \textsc{scikit-learn}\footnote{Visited at 03.04.2023\\ \url{https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html}}.
In addition to that we use the implementation of the variance inflation factor implemented in the \textsc{statsmodels} library\footnote{Visited at 03.04.2023\\ \url{https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html}}.
%%%%%%%%%%%%%%%%

\input{Chapters/Thesis/Experiments.tex}